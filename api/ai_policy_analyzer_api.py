#!/usr/bin/env python3
"""
AI Policy Analyzer and Conflict Predictor API - Chapter 17.1
============================================================
Tasks 17.1-T48, T49: AI-powered policy analysis and conflict prediction

Features:
- AI Policy Analyzer Agent (GPT-4 powered policy explanation)
- AI Policy Conflict Predictor (ML model for conflict detection)
- Policy recommendation engine
- Natural language policy queries
- Policy impact assessment
- Cross-framework conflict detection
"""

from fastapi import APIRouter, HTTPException, Depends, Query, Body, BackgroundTasks
from pydantic import BaseModel, Field, validator
from typing import Dict, List, Any, Optional, Union
from datetime import datetime, timedelta
from enum import Enum
import logging
import uuid
import json
import asyncio
import openai
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pickle

from src.services.connection_pool_manager import get_pool_manager

logger = logging.getLogger(__name__)
router = APIRouter()

# =====================================================
# PYDANTIC MODELS
# =====================================================

class PolicyAnalysisRequest(BaseModel):
    """Request for AI policy analysis"""
    policy_pack_id: str = Field(..., description="Policy pack to analyze")
    analysis_type: str = Field("EXPLAIN", description="EXPLAIN, IMPACT, CONFLICTS, RECOMMENDATIONS")
    natural_language_query: Optional[str] = Field(None, description="Natural language question about policy")
    context: Dict[str, Any] = Field(default_factory=dict, description="Additional context")
    tenant_id: int = Field(..., description="Tenant identifier")
    user_id: str = Field(..., description="Requesting user ID")

class PolicyConflictPredictionRequest(BaseModel):
    """Request for policy conflict prediction"""
    policy_pack_ids: List[str] = Field(..., description="Policy packs to analyze for conflicts")
    workflow_context: Dict[str, Any] = Field(..., description="Workflow execution context")
    tenant_id: int = Field(..., description="Tenant identifier")
    industry: str = Field(..., description="Industry context")
    compliance_frameworks: List[str] = Field(default_factory=list, description="Active compliance frameworks")

class PolicyRecommendationRequest(BaseModel):
    """Request for policy recommendations"""
    industry: str = Field(..., description="Target industry")
    compliance_requirements: List[str] = Field(..., description="Required compliance frameworks")
    business_context: Dict[str, Any] = Field(..., description="Business context and requirements")
    tenant_id: int = Field(..., description="Tenant identifier")
    risk_tolerance: str = Field("MEDIUM", description="LOW, MEDIUM, HIGH")

class ConflictType(str, Enum):
    RULE_CONTRADICTION = "rule_contradiction"
    PRECEDENCE_CONFLICT = "precedence_conflict"
    FRAMEWORK_OVERLAP = "framework_overlap"
    ENFORCEMENT_MISMATCH = "enforcement_mismatch"
    SCOPE_COLLISION = "scope_collision"

class AnalysisType(str, Enum):
    EXPLAIN = "EXPLAIN"
    IMPACT = "IMPACT"
    CONFLICTS = "CONFLICTS"
    RECOMMENDATIONS = "RECOMMENDATIONS"

# =====================================================
# AI POLICY ANALYZER SERVICE
# =====================================================

class AIPolicyAnalyzerService:
    """
    AI-powered Policy Analysis Service
    Tasks 17.1-T48, T49: AI policy analyzer and conflict predictor
    """
    
    def __init__(self, pool_manager):
        self.pool_manager = pool_manager
        self.logger = logging.getLogger(__name__)
        
        # Initialize OpenAI client
        self.openai_client = openai.AsyncOpenAI(
            api_key=os.getenv('OPENAI_API_KEY', 'dummy-key')
        )
        
        # Initialize ML models for conflict prediction
        self.conflict_predictor = None
        self.policy_vectorizer = None
        self._initialize_ml_models()
        
        # Policy analysis templates
        self.analysis_templates = {
            'EXPLAIN': self._get_explanation_template(),
            'IMPACT': self._get_impact_assessment_template(),
            'CONFLICTS': self._get_conflict_analysis_template(),
            'RECOMMENDATIONS': self._get_recommendation_template()
        }
        
        # Industry-specific policy knowledge
        self.industry_knowledge = {
            'SaaS': {
                'common_frameworks': ['SOX', 'GDPR', 'SOC2'],
                'key_risks': ['data_privacy', 'revenue_recognition', 'customer_churn'],
                'typical_policies': ['subscription_lifecycle', 'billing_accuracy', 'data_retention']
            },
            'BFSI': {
                'common_frameworks': ['RBI', 'BASEL_III', 'AML'],
                'key_risks': ['credit_risk', 'operational_risk', 'compliance_risk'],
                'typical_policies': ['kyc_verification', 'loan_approval', 'fraud_detection']
            },
            'Insurance': {
                'common_frameworks': ['IRDAI', 'SOLVENCY_II', 'GDPR'],
                'key_risks': ['underwriting_risk', 'claims_fraud', 'regulatory_compliance'],
                'typical_policies': ['claims_processing', 'premium_calculation', 'risk_assessment']
            }
        }
    
    def _initialize_ml_models(self):
        """Initialize ML models for conflict prediction"""
        try:
            # Try to load pre-trained models
            model_path = os.path.join(os.path.dirname(__file__), '..', 'models')
            
            if os.path.exists(os.path.join(model_path, 'conflict_predictor.pkl')):
                with open(os.path.join(model_path, 'conflict_predictor.pkl'), 'rb') as f:
                    self.conflict_predictor = pickle.load(f)
                
                with open(os.path.join(model_path, 'policy_vectorizer.pkl'), 'rb') as f:
                    self.policy_vectorizer = pickle.load(f)
                
                self.logger.info("âœ… Loaded pre-trained conflict prediction models")
            else:
                # Initialize new models
                self.conflict_predictor = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42
                )
                self.policy_vectorizer = TfidfVectorizer(
                    max_features=1000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                
                # Train with synthetic data (in production, use real historical data)
                self._train_initial_models()
                
                self.logger.info("âœ… Initialized new conflict prediction models")
                
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize ML models: {e}")
            # Fallback to rule-based conflict detection
            self.conflict_predictor = None
            self.policy_vectorizer = None
    
    async def analyze_policy(self, request: PolicyAnalysisRequest) -> Dict[str, Any]:
        """
        Task 17.1-T48: Build AI policy analyzer agent (explain policies)
        """
        try:
            analysis_id = str(uuid.uuid4())
            self.logger.info(f"ðŸ¤– Starting AI policy analysis: {request.analysis_type} for {request.policy_pack_id}")
            
            # Step 1: Get policy pack details
            policy_pack = await self._get_policy_pack(request.policy_pack_id)
            if not policy_pack:
                raise HTTPException(status_code=404, detail="Policy pack not found")
            
            # Step 2: Perform AI analysis based on type
            if request.analysis_type == AnalysisType.EXPLAIN:
                analysis_result = await self._explain_policy(policy_pack, request)
            elif request.analysis_type == AnalysisType.IMPACT:
                analysis_result = await self._assess_policy_impact(policy_pack, request)
            elif request.analysis_type == AnalysisType.CONFLICTS:
                analysis_result = await self._analyze_policy_conflicts(policy_pack, request)
            elif request.analysis_type == AnalysisType.RECOMMENDATIONS:
                analysis_result = await self._generate_policy_recommendations(policy_pack, request)
            else:
                raise HTTPException(status_code=400, detail=f"Unsupported analysis type: {request.analysis_type}")
            
            # Step 3: Store analysis results
            await self._store_analysis_results(analysis_id, request, analysis_result)
            
            return {
                "analysis_id": analysis_id,
                "policy_pack_id": request.policy_pack_id,
                "analysis_type": request.analysis_type,
                "results": analysis_result,
                "timestamp": datetime.utcnow().isoformat(),
                "confidence_score": analysis_result.get('confidence_score', 0.85)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ AI policy analysis failed: {e}")
            raise HTTPException(status_code=500, detail=f"Policy analysis failed: {str(e)}")
    
    async def predict_policy_conflicts(self, request: PolicyConflictPredictionRequest) -> Dict[str, Any]:
        """
        Task 17.1-T49: Build AI policy conflict predictor (ML model)
        """
        try:
            prediction_id = str(uuid.uuid4())
            self.logger.info(f"ðŸ”® Predicting policy conflicts for {len(request.policy_pack_ids)} policy packs")
            
            # Step 1: Get all policy packs
            policy_packs = []
            for pack_id in request.policy_pack_ids:
                pack = await self._get_policy_pack(pack_id)
                if pack:
                    policy_packs.append(pack)
            
            if len(policy_packs) < 2:
                return {
                    "prediction_id": prediction_id,
                    "conflicts_predicted": [],
                    "conflict_probability": 0.0,
                    "message": "Need at least 2 policy packs for conflict prediction"
                }
            
            # Step 2: Use ML model for conflict prediction
            if self.conflict_predictor and self.policy_vectorizer:
                ml_conflicts = await self._ml_conflict_prediction(policy_packs, request)
            else:
                ml_conflicts = []
            
            # Step 3: Use rule-based conflict detection as fallback/supplement
            rule_conflicts = await self._rule_based_conflict_detection(policy_packs, request)
            
            # Step 4: Combine and rank conflicts
            all_conflicts = ml_conflicts + rule_conflicts
            ranked_conflicts = await self._rank_conflicts(all_conflicts, request)
            
            # Step 5: Calculate overall conflict probability
            conflict_probability = min(len(ranked_conflicts) * 0.2, 1.0) if ranked_conflicts else 0.0
            
            # Step 6: Generate mitigation recommendations
            mitigation_recommendations = await self._generate_conflict_mitigations(ranked_conflicts)
            
            return {
                "prediction_id": prediction_id,
                "policy_pack_ids": request.policy_pack_ids,
                "conflicts_predicted": ranked_conflicts,
                "conflict_probability": conflict_probability,
                "total_conflicts_found": len(ranked_conflicts),
                "high_severity_conflicts": len([c for c in ranked_conflicts if c.get('severity') == 'HIGH']),
                "mitigation_recommendations": mitigation_recommendations,
                "prediction_confidence": 0.82,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Policy conflict prediction failed: {e}")
            raise HTTPException(status_code=500, detail=f"Conflict prediction failed: {str(e)}")
    
    async def generate_policy_recommendations(self, request: PolicyRecommendationRequest) -> Dict[str, Any]:
        """
        Generate AI-powered policy recommendations for industry and compliance requirements
        """
        try:
            recommendation_id = str(uuid.uuid4())
            self.logger.info(f"ðŸ’¡ Generating policy recommendations for {request.industry} industry")
            
            # Step 1: Get industry-specific knowledge
            industry_info = self.industry_knowledge.get(request.industry, {})
            
            # Step 2: Analyze compliance requirements
            compliance_analysis = await self._analyze_compliance_requirements(request)
            
            # Step 3: Generate AI-powered recommendations
            ai_recommendations = await self._generate_ai_recommendations(request, industry_info, compliance_analysis)
            
            # Step 4: Validate recommendations against existing policies
            validated_recommendations = await self._validate_recommendations(ai_recommendations, request)
            
            return {
                "recommendation_id": recommendation_id,
                "industry": request.industry,
                "compliance_requirements": request.compliance_requirements,
                "recommendations": validated_recommendations,
                "industry_insights": industry_info,
                "compliance_analysis": compliance_analysis,
                "implementation_priority": await self._prioritize_recommendations(validated_recommendations),
                "estimated_implementation_time": "2-4 weeks",
                "confidence_score": 0.88,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Policy recommendation generation failed: {e}")
            raise HTTPException(status_code=500, detail=f"Recommendation generation failed: {str(e)}")
    
    # AI Analysis Methods
    async def _explain_policy(self, policy_pack: Dict[str, Any], request: PolicyAnalysisRequest) -> Dict[str, Any]:
        """Use GPT-4 to explain policy pack in natural language"""
        try:
            # Prepare policy context
            policy_context = {
                "name": policy_pack['name'],
                "industry": policy_pack['industry'],
                "rules": json.loads(policy_pack['rules']),
                "compliance_frameworks": policy_pack['compliance_frameworks'],
                "enforcement_level": policy_pack['enforcement_level']
            }
            
            # Create GPT-4 prompt
            prompt = self._get_explanation_template().format(
                policy_name=policy_context['name'],
                industry=policy_context['industry'],
                rules=json.dumps(policy_context['rules'], indent=2),
                frameworks=', '.join(policy_context['compliance_frameworks']),
                enforcement=policy_context['enforcement_level'],
                user_query=request.natural_language_query or "Explain this policy pack"
            )
            
            # Call GPT-4
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert policy analyst specializing in compliance and governance frameworks."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.3
            )
            
            explanation = response.choices[0].message.content
            
            return {
                "explanation": explanation,
                "policy_summary": {
                    "total_rules": len(policy_context['rules']),
                    "enforcement_level": policy_context['enforcement_level'],
                    "compliance_frameworks": policy_context['compliance_frameworks'],
                    "industry_focus": policy_context['industry']
                },
                "key_requirements": await self._extract_key_requirements(policy_context['rules']),
                "business_impact": await self._assess_business_impact(policy_context),
                "confidence_score": 0.92
            }
            
        except Exception as e:
            self.logger.error(f"âŒ GPT-4 policy explanation failed: {e}")
            # Fallback to rule-based explanation
            return await self._rule_based_explanation(policy_pack)
    
    async def _assess_policy_impact(self, policy_pack: Dict[str, Any], request: PolicyAnalysisRequest) -> Dict[str, Any]:
        """Assess the impact of implementing this policy pack"""
        rules = json.loads(policy_pack['rules'])
        
        impact_assessment = {
            "operational_impact": "MEDIUM",
            "compliance_impact": "HIGH",
            "performance_impact": "LOW",
            "user_experience_impact": "LOW",
            "cost_impact": "MEDIUM",
            "risk_reduction": "HIGH",
            "implementation_complexity": "MEDIUM"
        }
        
        # Analyze rules for impact
        restrictive_rules = len([r for r in rules if r.get('type') == 'restriction'])
        validation_rules = len([r for r in rules if r.get('type') == 'validation'])
        
        if restrictive_rules > 5:
            impact_assessment["operational_impact"] = "HIGH"
            impact_assessment["user_experience_impact"] = "MEDIUM"
        
        if validation_rules > 10:
            impact_assessment["performance_impact"] = "MEDIUM"
        
        return {
            "impact_assessment": impact_assessment,
            "affected_workflows": await self._identify_affected_workflows(policy_pack),
            "mitigation_strategies": await self._suggest_impact_mitigations(impact_assessment),
            "rollout_recommendations": await self._suggest_rollout_strategy(impact_assessment),
            "monitoring_requirements": await self._suggest_monitoring_requirements(policy_pack)
        }
    
    async def _analyze_policy_conflicts(self, policy_pack: Dict[str, Any], request: PolicyAnalysisRequest) -> Dict[str, Any]:
        """Analyze potential conflicts with other policies"""
        # Get other policy packs for the same tenant/industry
        async with self.pool_manager.get_pool().acquire() as conn:
            other_packs = await conn.fetch("""
                SELECT * FROM policy_packs 
                WHERE id != $1 AND (tenant_id = $2 OR tenant_id IS NULL)
                AND industry IN ($3, 'ALL') AND status = 'PUBLISHED'
            """, policy_pack['id'], request.tenant_id, policy_pack['industry'])
        
        conflicts = []
        for other_pack in other_packs:
            pack_conflicts = await self._detect_conflicts_between_packs(policy_pack, dict(other_pack))
            conflicts.extend(pack_conflicts)
        
        return {
            "conflicts_found": conflicts,
            "total_conflicts": len(conflicts),
            "high_severity_conflicts": len([c for c in conflicts if c.get('severity') == 'HIGH']),
            "conflict_types": list(set([c.get('type') for c in conflicts])),
            "resolution_suggestions": await self._suggest_conflict_resolutions(conflicts)
        }
    
    async def _generate_policy_recommendations(self, policy_pack: Dict[str, Any], request: PolicyAnalysisRequest) -> Dict[str, Any]:
        """Generate recommendations for improving the policy pack"""
        rules = json.loads(policy_pack['rules'])
        
        recommendations = []
        
        # Check for missing industry-standard rules
        industry_standards = self.industry_knowledge.get(policy_pack['industry'], {}).get('typical_policies', [])
        existing_rule_types = [r.get('type', '') for r in rules]
        
        for standard in industry_standards:
            if standard not in existing_rule_types:
                recommendations.append({
                    "type": "MISSING_RULE",
                    "priority": "MEDIUM",
                    "description": f"Consider adding {standard} rule for {policy_pack['industry']} industry compliance",
                    "suggested_rule": await self._generate_rule_template(standard, policy_pack['industry'])
                })
        
        # Check for overly restrictive rules
        for rule in rules:
            if rule.get('enforcement', 'STRICT') == 'STRICT' and rule.get('severity', 'MEDIUM') == 'LOW':
                recommendations.append({
                    "type": "OPTIMIZATION",
                    "priority": "LOW",
                    "description": f"Rule '{rule.get('name', 'unnamed')}' might be overly restrictive for low severity violations",
                    "suggestion": "Consider changing enforcement to ADVISORY for low severity rules"
                })
        
        return {
            "recommendations": recommendations,
            "total_recommendations": len(recommendations),
            "high_priority": len([r for r in recommendations if r.get('priority') == 'HIGH']),
            "optimization_score": max(0, 100 - len(recommendations) * 5),
            "next_steps": await self._suggest_next_steps(recommendations)
        }
    
    # ML-based Conflict Prediction
    async def _ml_conflict_prediction(self, policy_packs: List[Dict[str, Any]], request: PolicyConflictPredictionRequest) -> List[Dict[str, Any]]:
        """Use ML model to predict policy conflicts"""
        if not self.conflict_predictor or not self.policy_vectorizer:
            return []
        
        try:
            conflicts = []
            
            # Compare each pair of policy packs
            for i in range(len(policy_packs)):
                for j in range(i + 1, len(policy_packs)):
                    pack1, pack2 = policy_packs[i], policy_packs[j]
                    
                    # Extract features for ML model
                    features = await self._extract_conflict_features(pack1, pack2, request)
                    
                    # Vectorize policy text
                    policy_text = f"{json.dumps(json.loads(pack1['rules']))} {json.dumps(json.loads(pack2['rules']))}"
                    text_features = self.policy_vectorizer.transform([policy_text])
                    
                    # Combine features
                    combined_features = np.hstack([features.reshape(1, -1), text_features.toarray()])
                    
                    # Predict conflict probability
                    conflict_prob = self.conflict_predictor.predict_proba(combined_features)[0][1]
                    
                    if conflict_prob > 0.6:  # Threshold for conflict detection
                        conflicts.append({
                            "type": ConflictType.RULE_CONTRADICTION,
                            "policy_pack_1": pack1['id'],
                            "policy_pack_2": pack2['id'],
                            "severity": "HIGH" if conflict_prob > 0.8 else "MEDIUM",
                            "probability": float(conflict_prob),
                            "description": f"ML model detected potential conflict between {pack1['name']} and {pack2['name']}",
                            "detection_method": "ML_MODEL"
                        })
            
            return conflicts
            
        except Exception as e:
            self.logger.error(f"âŒ ML conflict prediction failed: {e}")
            return []
    
    async def _rule_based_conflict_detection(self, policy_packs: List[Dict[str, Any]], request: PolicyConflictPredictionRequest) -> List[Dict[str, Any]]:
        """Rule-based conflict detection as fallback"""
        conflicts = []
        
        for i in range(len(policy_packs)):
            for j in range(i + 1, len(policy_packs)):
                pack1, pack2 = policy_packs[i], policy_packs[j]
                
                # Check precedence conflicts
                if pack1['precedence_level'] == pack2['precedence_level']:
                    rules1 = json.loads(pack1['rules'])
                    rules2 = json.loads(pack2['rules'])
                    
                    # Check for contradictory rules
                    for rule1 in rules1:
                        for rule2 in rules2:
                            if await self._rules_contradict(rule1, rule2):
                                conflicts.append({
                                    "type": ConflictType.RULE_CONTRADICTION,
                                    "policy_pack_1": pack1['id'],
                                    "policy_pack_2": pack2['id'],
                                    "severity": "HIGH",
                                    "rule_1": rule1.get('name', 'unnamed'),
                                    "rule_2": rule2.get('name', 'unnamed'),
                                    "description": f"Contradictory rules detected: {rule1.get('name')} vs {rule2.get('name')}",
                                    "detection_method": "RULE_BASED"
                                })
                
                # Check enforcement level conflicts
                if pack1['enforcement_level'] != pack2['enforcement_level']:
                    conflicts.append({
                        "type": ConflictType.ENFORCEMENT_MISMATCH,
                        "policy_pack_1": pack1['id'],
                        "policy_pack_2": pack2['id'],
                        "severity": "MEDIUM",
                        "description": f"Enforcement level mismatch: {pack1['enforcement_level']} vs {pack2['enforcement_level']}",
                        "detection_method": "RULE_BASED"
                    })
        
        return conflicts
    
    # Helper Methods
    def _get_explanation_template(self) -> str:
        return """
        Analyze and explain the following policy pack in clear, business-friendly language:

        Policy Name: {policy_name}
        Industry: {industry}
        Enforcement Level: {enforcement}
        Compliance Frameworks: {frameworks}

        Policy Rules:
        {rules}

        User Question: {user_query}

        Please provide:
        1. A clear explanation of what this policy pack does
        2. Why these rules are important for the business
        3. What happens when rules are violated
        4. How this relates to compliance requirements
        5. Any potential business impact or operational considerations

        Keep the explanation accessible to non-technical stakeholders while being comprehensive.
        """
    
    def _get_impact_assessment_template(self) -> str:
        return "Assess the business and operational impact of implementing this policy pack..."
    
    def _get_conflict_analysis_template(self) -> str:
        return "Analyze potential conflicts between this policy pack and existing policies..."
    
    def _get_recommendation_template(self) -> str:
        return "Provide recommendations for optimizing this policy pack..."
    
    async def _get_policy_pack(self, policy_pack_id: str) -> Optional[Dict[str, Any]]:
        """Get policy pack from database"""
        async with self.pool_manager.get_pool().acquire() as conn:
            policy_pack = await conn.fetchrow("""
                SELECT * FROM policy_packs WHERE id = $1
            """, policy_pack_id)
            
            return dict(policy_pack) if policy_pack else None
    
    async def _store_analysis_results(self, analysis_id: str, request: PolicyAnalysisRequest, results: Dict[str, Any]):
        """Store analysis results for audit trail"""
        async with self.pool_manager.get_pool().acquire() as conn:
            await conn.execute("""
                INSERT INTO policy_analysis_results (
                    analysis_id, policy_pack_id, analysis_type, user_id, 
                    tenant_id, results, created_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            """,
            analysis_id, request.policy_pack_id, request.analysis_type,
            request.user_id, request.tenant_id, json.dumps(results),
            datetime.utcnow())
    
    def _train_initial_models(self):
        """Train initial ML models with synthetic data"""
        # This would be replaced with real training data in production
        # For now, create synthetic training data
        
        synthetic_features = np.random.rand(1000, 10)  # 10 features
        synthetic_labels = np.random.choice([0, 1], 1000, p=[0.7, 0.3])  # 30% conflicts
        
        # Train conflict predictor
        self.conflict_predictor.fit(synthetic_features, synthetic_labels)
        
        # Train text vectorizer
        synthetic_texts = [f"policy rule {i} with conditions and requirements" for i in range(1000)]
        self.policy_vectorizer.fit(synthetic_texts)
        
        self.logger.info("âœ… Trained initial ML models with synthetic data")
    
    async def _extract_conflict_features(self, pack1: Dict[str, Any], pack2: Dict[str, Any], request: PolicyConflictPredictionRequest) -> np.ndarray:
        """Extract features for ML conflict prediction"""
        features = []
        
        # Precedence level difference
        features.append(abs(pack1['precedence_level'] - pack2['precedence_level']))
        
        # Enforcement level compatibility (0 = compatible, 1 = incompatible)
        enforcement_compat = 0 if pack1['enforcement_level'] == pack2['enforcement_level'] else 1
        features.append(enforcement_compat)
        
        # Industry compatibility
        industry_compat = 0 if pack1['industry'] == pack2['industry'] or pack1['industry'] == 'ALL' or pack2['industry'] == 'ALL' else 1
        features.append(industry_compat)
        
        # Rule count difference
        rules1 = json.loads(pack1['rules'])
        rules2 = json.loads(pack2['rules'])
        features.append(abs(len(rules1) - len(rules2)))
        
        # Compliance framework overlap
        frameworks1 = set(pack1['compliance_frameworks'] or [])
        frameworks2 = set(pack2['compliance_frameworks'] or [])
        overlap = len(frameworks1.intersection(frameworks2))
        features.append(overlap)
        
        # Add more features as needed
        features.extend([0.0] * (10 - len(features)))  # Pad to 10 features
        
        return np.array(features[:10])
    
    async def _rules_contradict(self, rule1: Dict[str, Any], rule2: Dict[str, Any]) -> bool:
        """Check if two rules contradict each other"""
        # Simple contradiction detection
        if rule1.get('type') == rule2.get('type'):
            # Check for opposite conditions
            condition1 = rule1.get('condition', '')
            condition2 = rule2.get('condition', '')
            
            # Look for obvious contradictions
            if 'allow' in condition1.lower() and 'deny' in condition2.lower():
                return True
            if 'required' in condition1.lower() and 'forbidden' in condition2.lower():
                return True
        
        return False
    
    async def _rank_conflicts(self, conflicts: List[Dict[str, Any]], request: PolicyConflictPredictionRequest) -> List[Dict[str, Any]]:
        """Rank conflicts by severity and business impact"""
        def conflict_score(conflict):
            severity_scores = {'HIGH': 3, 'MEDIUM': 2, 'LOW': 1}
            base_score = severity_scores.get(conflict.get('severity', 'LOW'), 1)
            
            # Boost score for compliance-related conflicts
            if conflict.get('type') == ConflictType.FRAMEWORK_OVERLAP:
                base_score += 1
            
            # Boost score for ML-detected conflicts with high probability
            if conflict.get('detection_method') == 'ML_MODEL' and conflict.get('probability', 0) > 0.8:
                base_score += 1
            
            return base_score
        
        return sorted(conflicts, key=conflict_score, reverse=True)
    
    async def _generate_conflict_mitigations(self, conflicts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate mitigation strategies for detected conflicts"""
        mitigations = []
        
        for conflict in conflicts:
            if conflict.get('type') == ConflictType.RULE_CONTRADICTION:
                mitigations.append({
                    "conflict_id": conflict.get('policy_pack_1', '') + '_' + conflict.get('policy_pack_2', ''),
                    "strategy": "RULE_PRECEDENCE",
                    "description": "Establish clear precedence hierarchy between conflicting rules",
                    "implementation": "Update policy pack precedence levels or add exception handling"
                })
            elif conflict.get('type') == ConflictType.ENFORCEMENT_MISMATCH:
                mitigations.append({
                    "conflict_id": conflict.get('policy_pack_1', '') + '_' + conflict.get('policy_pack_2', ''),
                    "strategy": "ENFORCEMENT_ALIGNMENT",
                    "description": "Align enforcement levels or create context-specific enforcement rules",
                    "implementation": "Review business requirements and standardize enforcement approach"
                })
        
        return mitigations
    
    async def _rule_based_explanation(self, policy_pack: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback rule-based explanation when GPT-4 is unavailable"""
        rules = json.loads(policy_pack['rules'])
        
        return {
            "explanation": f"This policy pack '{policy_pack['name']}' contains {len(rules)} rules for {policy_pack['industry']} industry compliance.",
            "policy_summary": {
                "total_rules": len(rules),
                "enforcement_level": policy_pack['enforcement_level'],
                "compliance_frameworks": policy_pack['compliance_frameworks'],
                "industry_focus": policy_pack['industry']
            },
            "key_requirements": [rule.get('name', f'Rule {i+1}') for i, rule in enumerate(rules)],
            "business_impact": "Ensures compliance with industry regulations and governance requirements",
            "confidence_score": 0.65
        }
    
    async def _extract_key_requirements(self, rules: List[Dict[str, Any]]) -> List[str]:
        """Extract key requirements from policy rules"""
        requirements = []
        for rule in rules:
            if rule.get('required_fields'):
                requirements.append(f"Required fields: {', '.join(rule['required_fields'])}")
            if rule.get('validation_rules'):
                requirements.append(f"Validation: {rule['validation_rules']}")
        return requirements
    
    async def _assess_business_impact(self, policy_context: Dict[str, Any]) -> str:
        """Assess business impact of policy"""
        if policy_context['enforcement_level'] == 'STRICT':
            return "High impact - strict enforcement may block operations if violations occur"
        elif policy_context['enforcement_level'] == 'ADVISORY':
            return "Medium impact - violations logged but operations continue"
        else:
            return "Low impact - policy monitoring only"

# =====================================================
# API ENDPOINTS
# =====================================================

# Initialize service
ai_policy_service = None

async def get_ai_policy_service():
    global ai_policy_service
    if ai_policy_service is None:
        pool_manager = await get_pool_manager()
        ai_policy_service = AIPolicyAnalyzerService(pool_manager)
    return ai_policy_service

@router.post("/analyze-policy", response_model=Dict[str, Any])
async def analyze_policy(
    request: PolicyAnalysisRequest,
    service: AIPolicyAnalyzerService = Depends(get_ai_policy_service)
):
    """
    Task 17.1-T48: Build AI policy analyzer agent (explain policies)
    """
    return await service.analyze_policy(request)

@router.post("/predict-conflicts", response_model=Dict[str, Any])
async def predict_policy_conflicts(
    request: PolicyConflictPredictionRequest,
    service: AIPolicyAnalyzerService = Depends(get_ai_policy_service)
):
    """
    Task 17.1-T49: Build AI policy conflict predictor (ML model)
    """
    return await service.predict_policy_conflicts(request)

@router.post("/recommend-policies", response_model=Dict[str, Any])
async def recommend_policies(
    request: PolicyRecommendationRequest,
    service: AIPolicyAnalyzerService = Depends(get_ai_policy_service)
):
    """
    Generate AI-powered policy recommendations
    """
    return await service.generate_policy_recommendations(request)

@router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "ai_policy_analyzer",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "ai_models_loaded": ai_policy_service.conflict_predictor is not None if ai_policy_service else False
    }
