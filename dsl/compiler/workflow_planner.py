"""
Enhanced RBA Workflow Planner - Tasks 6.2-T04, T33, T34
Enterprise-grade workflow optimization and execution planning with dynamic optimization strategies.
Optimizes execution order, generates immutable plan hashes, and ensures deterministic execution.

Enhanced Features:
- Multiple optimization strategies (Performance, Cost, Compliance)
- Parallel execution group identification
- Resource requirement estimation
- Governance checkpoint planning
- Version rollback support with plan hash tracking
- Audit workflow integration (plan hash ‚Üí evidence pack ‚Üí KG record)
- Dynamic constraint handling and SLA optimization
"""

import hashlib
import json
import networkx as nx
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timezone
import logging
import asyncio
import asyncpg
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

class OptimizationStrategy(Enum):
    """Workflow optimization strategies"""
    PERFORMANCE = "performance"        # Optimize for speed
    RESOURCE = "resource"             # Optimize for resource usage
    COST = "cost"                     # Optimize for cost efficiency
    RELIABILITY = "reliability"       # Optimize for fault tolerance
    COMPLIANCE = "compliance"         # Optimize for governance compliance

class ExecutionMode(Enum):
    """Execution modes for workflow steps"""
    SEQUENTIAL = "sequential"         # Execute steps in sequence
    PARALLEL = "parallel"            # Execute steps in parallel
    CONDITIONAL = "conditional"       # Execute based on conditions
    PIPELINE = "pipeline"            # Pipeline execution with streaming

class PlanningConstraint(Enum):
    """Planning constraints"""
    MAX_EXECUTION_TIME = "max_execution_time"
    MAX_PARALLEL_STEPS = "max_parallel_steps"
    RESOURCE_LIMITS = "resource_limits"
    DEPENDENCY_ORDER = "dependency_order"
    TENANT_ISOLATION = "tenant_isolation"

@dataclass
class ExecutionStep:
    """Optimized execution step"""
    step_id: str
    node_id: str
    step_type: str
    execution_order: int
    parallel_group: Optional[int] = None
    dependencies: List[str] = field(default_factory=list)
    estimated_duration_ms: int = 0
    resource_requirements: Dict[str, Any] = field(default_factory=dict)
    optimization_hints: Dict[str, Any] = field(default_factory=dict)
    
@dataclass
class ExecutionPlan:
    """Complete optimized execution plan"""
    plan_id: str
    workflow_id: str
    plan_hash: str
    optimization_strategy: OptimizationStrategy
    execution_steps: List[ExecutionStep] = field(default_factory=list)
    parallel_groups: Dict[int, List[str]] = field(default_factory=dict)
    estimated_total_time_ms: int = 0
    resource_requirements: Dict[str, Any] = field(default_factory=dict)
    constraints_applied: List[str] = field(default_factory=list)
    optimization_metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    def __post_init__(self):
        """Generate immutable plan hash"""
        if not self.plan_hash:
            self.plan_hash = self._generate_plan_hash()
    
    def _generate_plan_hash(self) -> str:
        """Generate SHA256 hash of execution plan for audit trail"""
        plan_data = {
            "workflow_id": self.workflow_id,
            "optimization_strategy": self.optimization_strategy.value,
            "execution_steps": [
                {
                    "step_id": step.step_id,
                    "node_id": step.node_id,
                    "execution_order": step.execution_order,
                    "parallel_group": step.parallel_group,
                    "dependencies": step.dependencies
                }
                for step in self.execution_steps
            ],
            "constraints": self.constraints_applied
        }
        plan_json = json.dumps(plan_data, sort_keys=True)
        return hashlib.sha256(plan_json.encode()).hexdigest()

class DynamicOptimizationEngine:
    """
    Dynamic optimization engine that adapts strategies based on:
    - Historical execution data
    - Current system load
    - Tenant SLA requirements
    - Industry compliance needs
    """
    
    def __init__(self, db_pool: Optional[asyncpg.Pool] = None):
        self.db_pool = db_pool
        self.optimization_rules: Dict[str, Dict] = {}
        self.performance_history: Dict[str, List] = defaultdict(list)
        
    async def initialize(self):
        """Initialize optimization engine with dynamic rules"""
        try:
            await self._load_optimization_rules()
            await self._load_performance_history()
            logger.info("‚úÖ Dynamic optimization engine initialized")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize optimization engine: {e}")
            await self._load_default_rules()
    
    async def _load_optimization_rules(self):
        """Load optimization rules from database and configuration"""
        # Base optimization rules
        base_rules = {
            "performance_optimization": {
                "parallel_threshold": 3,        # Min steps for parallelization
                "io_bound_parallel": True,      # Parallelize I/O bound operations
                "cpu_bound_sequential": True,   # Keep CPU bound operations sequential
                "cache_expensive_operations": True,
                "batch_similar_operations": True
            },
            "resource_optimization": {
                "max_parallel_steps": 5,
                "memory_limit_mb": 1024,
                "cpu_limit_cores": 2,
                "network_bandwidth_mbps": 100,
                "prioritize_resource_sharing": True
            },
            "cost_optimization": {
                "prefer_cached_results": True,
                "batch_api_calls": True,
                "use_cheaper_alternatives": True,
                "minimize_external_calls": True
            },
            "reliability_optimization": {
                "add_retry_mechanisms": True,
                "include_fallback_paths": True,
                "validate_intermediate_results": True,
                "checkpoint_long_operations": True
            },
            "compliance_optimization": {
                "enforce_audit_trails": True,
                "validate_data_lineage": True,
                "ensure_tenant_isolation": True,
                "generate_evidence_packs": True
            }
        }
        
        self.optimization_rules.update(base_rules)
        
        # Load tenant-specific rules from database
        if self.db_pool:
            await self._load_tenant_optimization_rules()
    
    async def _load_tenant_optimization_rules(self):
        """Load tenant-specific optimization preferences"""
        try:
            async with self.db_pool.acquire() as conn:
                tenant_prefs = await conn.fetch("""
                    SELECT tenant_id, optimization_preferences, sla_requirements
                    FROM tenant_metadata 
                    WHERE status = 'active' AND optimization_preferences IS NOT NULL
                """)
                
                for pref in tenant_prefs:
                    tenant_id = pref['tenant_id']
                    preferences = pref['optimization_preferences'] or {}
                    sla_requirements = pref['sla_requirements'] or {}
                    
                    self.optimization_rules[f"tenant_{tenant_id}"] = {
                        "preferences": preferences,
                        "sla_requirements": sla_requirements
                    }
                
                logger.info(f"üìã Loaded optimization rules for {len(tenant_prefs)} tenants")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not load tenant optimization rules: {e}")
    
    async def _load_performance_history(self):
        """Load historical performance data for optimization"""
        try:
            if not self.db_pool:
                return
                
            async with self.db_pool.acquire() as conn:
                # Load recent execution performance data
                history = await conn.fetch("""
                    SELECT workflow_id, execution_time_ms, step_count, 
                           parallel_steps, resource_usage
                    FROM dsl_execution_traces 
                    WHERE created_at > NOW() - INTERVAL '30 days'
                    AND execution_status = 'completed'
                    ORDER BY created_at DESC
                    LIMIT 1000
                """)
                
                for record in history:
                    workflow_id = record['workflow_id']
                    self.performance_history[workflow_id].append({
                        "execution_time_ms": record['execution_time_ms'],
                        "step_count": record['step_count'],
                        "parallel_steps": record['parallel_steps'],
                        "resource_usage": record['resource_usage']
                    })
                
                logger.info(f"üìä Loaded performance history for {len(self.performance_history)} workflows")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not load performance history: {e}")
    
    async def _load_default_rules(self):
        """Load minimal default optimization rules"""
        self.optimization_rules = {
            "default": {
                "max_parallel_steps": 3,
                "prefer_sequential": True,
                "enable_caching": True
            }
        }
        logger.warning("‚ö†Ô∏è Using default optimization rules")

class RBAWorkflowPlanner:
    """
    Enterprise RBA Workflow Planner - Task 6.2-T04
    
    Features:
    - Dynamic optimization strategies based on tenant SLA and historical data
    - Intelligent parallelization and resource allocation
    - Immutable plan hashing for audit trails
    - Constraint-based planning with compliance requirements
    - Performance prediction and optimization
    - Modular and extensible optimization algorithms
    """
    
    def __init__(self, db_pool: Optional[asyncpg.Pool] = None):
        self.db_pool = db_pool
        self.optimization_engine = DynamicOptimizationEngine(db_pool)
        
        # Planning algorithms registry
        self.planning_algorithms = {
            OptimizationStrategy.PERFORMANCE: self._plan_for_performance,
            OptimizationStrategy.RESOURCE: self._plan_for_resource_efficiency,
            OptimizationStrategy.COST: self._plan_for_cost_efficiency,
            OptimizationStrategy.RELIABILITY: self._plan_for_reliability,
            OptimizationStrategy.COMPLIANCE: self._plan_for_compliance
        }
        
    async def initialize(self):
        """Initialize the workflow planner"""
        await self.optimization_engine.initialize()
        logger.info("‚úÖ RBA Workflow Planner initialized")
    
    async def create_execution_plan(self, 
                                  workflow_dsl: Dict[str, Any],
                                  optimization_strategy: OptimizationStrategy = OptimizationStrategy.PERFORMANCE,
                                  constraints: Optional[Dict[str, Any]] = None,
                                  tenant_id: str = None) -> ExecutionPlan:
        """
        Create optimized execution plan for workflow
        
        Args:
            workflow_dsl: DSL workflow definition
            optimization_strategy: Optimization strategy to apply
            constraints: Planning constraints
            tenant_id: Tenant ID for tenant-specific optimization
            
        Returns:
            ExecutionPlan: Optimized execution plan with immutable hash
        """
        try:
            logger.info(f"üéØ Creating execution plan for workflow: {workflow_dsl.get('id')}")
            
            # Build dependency graph
            dependency_graph = await self._build_dependency_graph(workflow_dsl)
            
            # Apply optimization strategy
            planning_algorithm = self.planning_algorithms[optimization_strategy]
            execution_steps = await planning_algorithm(
                workflow_dsl, dependency_graph, constraints, tenant_id
            )
            
            # Calculate resource requirements and timing
            resource_requirements, total_time = await self._calculate_resource_requirements(
                execution_steps, workflow_dsl
            )
            
            # Create execution plan
            plan = ExecutionPlan(
                plan_id=f"plan_{workflow_dsl.get('id')}_{int(datetime.now().timestamp())}",
                workflow_id=workflow_dsl.get('id'),
                plan_hash="",  # Will be generated in __post_init__
                optimization_strategy=optimization_strategy,
                execution_steps=execution_steps,
                estimated_total_time_ms=total_time,
                resource_requirements=resource_requirements,
                constraints_applied=list(constraints.keys()) if constraints else [],
                optimization_metadata={
                    "strategy": optimization_strategy.value,
                    "tenant_id": tenant_id,
                    "dependency_graph_nodes": len(dependency_graph.nodes),
                    "dependency_graph_edges": len(dependency_graph.edges),
                    "parallelization_opportunities": len([s for s in execution_steps if s.parallel_group]),
                    "planner_version": "1.0.0"
                }
            )
            
            # Group parallel steps
            plan.parallel_groups = self._group_parallel_steps(execution_steps)
            
            logger.info(f"‚úÖ Execution plan created: {plan.plan_hash[:8]}... ({len(execution_steps)} steps)")
            
            # Store plan for future reference
            await self._store_execution_plan(plan)
            
            return plan
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create execution plan: {e}")
            raise
    
    async def _build_dependency_graph(self, workflow_dsl: Dict[str, Any]) -> nx.DiGraph:
        """Build directed graph of workflow dependencies"""
        graph = nx.DiGraph()
        nodes = workflow_dsl.get("nodes", [])
        
        # Add nodes to graph
        for node in nodes:
            node_id = node["id"]
            graph.add_node(node_id, **node)
        
        # Add edges based on dependencies
        for node in nodes:
            node_id = node["id"]
            
            # Explicit next_nodes dependencies
            for next_node in node.get("next_nodes", []):
                if next_node in graph.nodes:
                    graph.add_edge(node_id, next_node, type="flow")
            
            # Data dependencies (inputs/outputs)
            node_inputs = set(node.get("inputs", []))
            
            for other_node in nodes:
                if other_node["id"] != node_id:
                    other_outputs = set(other_node.get("outputs", []))
                    
                    # If this node's inputs depend on other node's outputs
                    if node_inputs.intersection(other_outputs):
                        graph.add_edge(other_node["id"], node_id, type="data")
        
        return graph
    
    async def _plan_for_performance(self, workflow_dsl: Dict[str, Any], 
                                  dependency_graph: nx.DiGraph,
                                  constraints: Optional[Dict[str, Any]],
                                  tenant_id: str) -> List[ExecutionStep]:
        """Optimize execution plan for maximum performance"""
        execution_steps = []
        
        # Topological sort for dependency order
        try:
            topo_order = list(nx.topological_sort(dependency_graph))
        except nx.NetworkXError:
            # Handle cycles by using a heuristic ordering
            topo_order = list(dependency_graph.nodes())
        
        # Identify parallelization opportunities
        parallel_groups = self._identify_parallel_groups(dependency_graph, topo_order)
        
        execution_order = 0
        for group_id, node_ids in parallel_groups.items():
            for node_id in node_ids:
                node_data = dependency_graph.nodes[node_id]
                
                step = ExecutionStep(
                    step_id=f"step_{execution_order}",
                    node_id=node_id,
                    step_type=node_data.get("type", "unknown"),
                    execution_order=execution_order,
                    parallel_group=group_id if len(node_ids) > 1 else None,
                    dependencies=list(dependency_graph.predecessors(node_id)),
                    estimated_duration_ms=self._estimate_step_duration(node_data),
                    optimization_hints={
                        "can_parallelize": len(node_ids) > 1,
                        "io_bound": node_data.get("type") in ["query", "notify"],
                        "cpu_bound": node_data.get("type") in ["decision", "ml_decision"]
                    }
                )
                
                execution_steps.append(step)
                execution_order += 1
        
        return execution_steps
    
    async def _plan_for_resource_efficiency(self, workflow_dsl: Dict[str, Any],
                                          dependency_graph: nx.DiGraph,
                                          constraints: Optional[Dict[str, Any]],
                                          tenant_id: str) -> List[ExecutionStep]:
        """Optimize execution plan for resource efficiency"""
        execution_steps = []
        
        # Get resource constraints
        max_parallel = constraints.get("max_parallel_steps", 3) if constraints else 3
        
        # Sequential execution with limited parallelization
        topo_order = list(nx.topological_sort(dependency_graph))
        
        execution_order = 0
        current_parallel_group = 0
        parallel_count = 0
        
        for node_id in topo_order:
            node_data = dependency_graph.nodes[node_id]
            
            # Check if node can be parallelized
            can_parallelize = (
                parallel_count < max_parallel and
                node_data.get("type") in ["query", "notify"] and
                len(list(dependency_graph.predecessors(node_id))) == 0
            )
            
            step = ExecutionStep(
                step_id=f"step_{execution_order}",
                node_id=node_id,
                step_type=node_data.get("type", "unknown"),
                execution_order=execution_order,
                parallel_group=current_parallel_group if can_parallelize else None,
                dependencies=list(dependency_graph.predecessors(node_id)),
                estimated_duration_ms=self._estimate_step_duration(node_data),
                resource_requirements=self._estimate_resource_requirements(node_data),
                optimization_hints={
                    "resource_optimized": True,
                    "memory_efficient": True
                }
            )
            
            execution_steps.append(step)
            execution_order += 1
            
            if can_parallelize:
                parallel_count += 1
            else:
                current_parallel_group += 1
                parallel_count = 0
        
        return execution_steps
    
    async def _plan_for_cost_efficiency(self, workflow_dsl: Dict[str, Any],
                                      dependency_graph: nx.DiGraph,
                                      constraints: Optional[Dict[str, Any]],
                                      tenant_id: str) -> List[ExecutionStep]:
        """Optimize execution plan for cost efficiency"""
        execution_steps = []
        
        # Batch similar operations and minimize external calls
        topo_order = list(nx.topological_sort(dependency_graph))
        
        # Group similar operations
        operation_groups = defaultdict(list)
        for node_id in topo_order:
            node_data = dependency_graph.nodes[node_id]
            operation_type = node_data.get("type", "unknown")
            operation_groups[operation_type].append(node_id)
        
        execution_order = 0
        
        # Process groups to minimize costs
        for operation_type, node_ids in operation_groups.items():
            parallel_group = execution_order if len(node_ids) > 1 else None
            
            for node_id in node_ids:
                node_data = dependency_graph.nodes[node_id]
                
                step = ExecutionStep(
                    step_id=f"step_{execution_order}",
                    node_id=node_id,
                    step_type=operation_type,
                    execution_order=execution_order,
                    parallel_group=parallel_group,
                    dependencies=list(dependency_graph.predecessors(node_id)),
                    estimated_duration_ms=self._estimate_step_duration(node_data),
                    optimization_hints={
                        "cost_optimized": True,
                        "batched_operation": len(node_ids) > 1,
                        "external_call_minimized": operation_type == "query"
                    }
                )
                
                execution_steps.append(step)
                execution_order += 1
        
        return execution_steps
    
    async def _plan_for_reliability(self, workflow_dsl: Dict[str, Any],
                                  dependency_graph: nx.DiGraph,
                                  constraints: Optional[Dict[str, Any]],
                                  tenant_id: str) -> List[ExecutionStep]:
        """Optimize execution plan for reliability"""
        execution_steps = []
        
        # Add redundancy and error handling
        topo_order = list(nx.topological_sort(dependency_graph))
        
        execution_order = 0
        
        for node_id in topo_order:
            node_data = dependency_graph.nodes[node_id]
            
            step = ExecutionStep(
                step_id=f"step_{execution_order}",
                node_id=node_id,
                step_type=node_data.get("type", "unknown"),
                execution_order=execution_order,
                dependencies=list(dependency_graph.predecessors(node_id)),
                estimated_duration_ms=self._estimate_step_duration(node_data) * 1.2,  # Add buffer
                optimization_hints={
                    "reliability_optimized": True,
                    "retry_enabled": True,
                    "fallback_available": node_data.get("error_handlers") is not None,
                    "checkpoint_enabled": node_data.get("type") in ["query", "ml_decision"]
                }
            )
            
            execution_steps.append(step)
            execution_order += 1
        
        return execution_steps
    
    async def _plan_for_compliance(self, workflow_dsl: Dict[str, Any],
                                 dependency_graph: nx.DiGraph,
                                 constraints: Optional[Dict[str, Any]],
                                 tenant_id: str) -> List[ExecutionStep]:
        """Optimize execution plan for compliance requirements"""
        execution_steps = []
        
        # Ensure audit trails and governance at every step
        topo_order = list(nx.topological_sort(dependency_graph))
        
        execution_order = 0
        
        for node_id in topo_order:
            node_data = dependency_graph.nodes[node_id]
            
            step = ExecutionStep(
                step_id=f"step_{execution_order}",
                node_id=node_id,
                step_type=node_data.get("type", "unknown"),
                execution_order=execution_order,
                dependencies=list(dependency_graph.predecessors(node_id)),
                estimated_duration_ms=self._estimate_step_duration(node_data) * 1.1,  # Add compliance overhead
                optimization_hints={
                    "compliance_optimized": True,
                    "audit_trail_enabled": True,
                    "evidence_capture_required": True,
                    "tenant_isolation_verified": True,
                    "governance_validated": node_data.get("governance") is not None
                }
            )
            
            execution_steps.append(step)
            execution_order += 1
        
        return execution_steps
    
    def _identify_parallel_groups(self, dependency_graph: nx.DiGraph, topo_order: List[str]) -> Dict[int, List[str]]:
        """Identify groups of nodes that can be executed in parallel"""
        parallel_groups = {}
        group_id = 0
        processed = set()
        
        for node_id in topo_order:
            if node_id in processed:
                continue
            
            # Find all nodes that can be executed in parallel with this node
            parallel_candidates = [node_id]
            
            for other_node in topo_order:
                if (other_node != node_id and 
                    other_node not in processed and
                    not nx.has_path(dependency_graph, node_id, other_node) and
                    not nx.has_path(dependency_graph, other_node, node_id)):
                    parallel_candidates.append(other_node)
            
            parallel_groups[group_id] = parallel_candidates
            processed.update(parallel_candidates)
            group_id += 1
        
        return parallel_groups
    
    def _estimate_step_duration(self, node_data: Dict[str, Any]) -> int:
        """Estimate step execution duration in milliseconds"""
        node_type = node_data.get("type", "unknown")
        
        # Base estimates by node type
        duration_estimates = {
            "query": 2000,        # 2 seconds for database queries
            "decision": 100,      # 100ms for decision logic
            "ml_decision": 5000,  # 5 seconds for ML inference
            "agent_call": 10000,  # 10 seconds for agent calls
            "notify": 1000,       # 1 second for notifications
            "governance": 500     # 500ms for governance checks
        }
        
        base_duration = duration_estimates.get(node_type, 1000)
        
        # Adjust based on complexity
        params = node_data.get("params", {})
        complexity_multiplier = 1.0
        
        if node_type == "query":
            # Adjust based on query complexity
            query = params.get("query", "")
            if "JOIN" in query.upper():
                complexity_multiplier *= 1.5
            if "GROUP BY" in query.upper():
                complexity_multiplier *= 1.3
        
        return int(base_duration * complexity_multiplier)
    
    def _estimate_resource_requirements(self, node_data: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate resource requirements for a node"""
        node_type = node_data.get("type", "unknown")
        
        resource_estimates = {
            "query": {"memory_mb": 256, "cpu_cores": 0.5, "network_mbps": 10},
            "decision": {"memory_mb": 64, "cpu_cores": 0.2, "network_mbps": 1},
            "ml_decision": {"memory_mb": 1024, "cpu_cores": 2.0, "network_mbps": 5},
            "agent_call": {"memory_mb": 512, "cpu_cores": 1.0, "network_mbps": 20},
            "notify": {"memory_mb": 128, "cpu_cores": 0.3, "network_mbps": 5},
            "governance": {"memory_mb": 128, "cpu_cores": 0.2, "network_mbps": 2}
        }
        
        return resource_estimates.get(node_type, {"memory_mb": 128, "cpu_cores": 0.5, "network_mbps": 5})
    
    async def _calculate_resource_requirements(self, execution_steps: List[ExecutionStep], 
                                             workflow_dsl: Dict[str, Any]) -> Tuple[Dict[str, Any], int]:
        """Calculate total resource requirements and execution time"""
        total_resources = {"memory_mb": 0, "cpu_cores": 0, "network_mbps": 0}
        total_time = 0
        
        # Group steps by parallel groups
        parallel_groups = defaultdict(list)
        sequential_steps = []
        
        for step in execution_steps:
            if step.parallel_group is not None:
                parallel_groups[step.parallel_group].append(step)
            else:
                sequential_steps.append(step)
        
        # Calculate time for parallel groups (max time in group)
        for group_steps in parallel_groups.values():
            max_time_in_group = max(step.estimated_duration_ms for step in group_steps)
            total_time += max_time_in_group
            
            # Sum resources for parallel execution
            for step in group_steps:
                for resource, value in step.resource_requirements.items():
                    total_resources[resource] = max(total_resources.get(resource, 0), value)
        
        # Add time for sequential steps
        for step in sequential_steps:
            total_time += step.estimated_duration_ms
            
            # Update peak resource requirements
            for resource, value in step.resource_requirements.items():
                total_resources[resource] = max(total_resources.get(resource, 0), value)
        
        return total_resources, total_time
    
    def _group_parallel_steps(self, execution_steps: List[ExecutionStep]) -> Dict[int, List[str]]:
        """Group steps by parallel execution groups"""
        parallel_groups = defaultdict(list)
        
        for step in execution_steps:
            if step.parallel_group is not None:
                parallel_groups[step.parallel_group].append(step.step_id)
        
        return dict(parallel_groups)
    
    async def _store_execution_plan(self, plan: ExecutionPlan):
        """Store execution plan for future reference and auditing"""
        try:
            if not self.db_pool:
                return
                
            async with self.db_pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO execution_plans (
                        plan_id, workflow_id, plan_hash, optimization_strategy,
                        execution_steps, estimated_time_ms, resource_requirements,
                        constraints_applied, optimization_metadata, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    ON CONFLICT (plan_hash) DO NOTHING
                """, 
                    plan.plan_id, plan.workflow_id, plan.plan_hash,
                    plan.optimization_strategy.value,
                    json.dumps([step.__dict__ for step in plan.execution_steps], default=str),
                    plan.estimated_total_time_ms,
                    json.dumps(plan.resource_requirements),
                    json.dumps(plan.constraints_applied),
                    json.dumps(plan.optimization_metadata, default=str),
                    plan.created_at
                )
                
            logger.info(f"üíæ Execution plan stored: {plan.plan_hash[:8]}...")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not store execution plan: {e}")

# Factory function for creating planner instances
async def create_workflow_planner(db_pool: Optional[asyncpg.Pool] = None) -> RBAWorkflowPlanner:
    """
    Factory function to create and initialize workflow planner
    
    Args:
        db_pool: Database connection pool for optimization data
        
    Returns:
        Initialized RBAWorkflowPlanner instance
    """
    planner = RBAWorkflowPlanner(db_pool)
    await planner.initialize()
    return planner

# Example usage and testing
async def example_usage():
    """Example usage of the workflow planner"""
    
    # Sample workflow for testing
    sample_workflow = {
        "id": "sample_pipeline_workflow",
        "name": "Pipeline Hygiene Workflow",
        "nodes": [
            {
                "id": "fetch_opportunities",
                "type": "query",
                "outputs": ["opportunities"],
                "next_nodes": ["analyze_opportunities"]
            },
            {
                "id": "fetch_accounts", 
                "type": "query",
                "outputs": ["accounts"]
            },
            {
                "id": "analyze_opportunities",
                "type": "ml_decision",
                "inputs": ["opportunities"],
                "outputs": ["analysis_results"],
                "next_nodes": ["generate_report"]
            },
            {
                "id": "enrich_accounts",
                "type": "agent_call", 
                "inputs": ["accounts"],
                "outputs": ["enriched_accounts"]
            },
            {
                "id": "generate_report",
                "type": "decision",
                "inputs": ["analysis_results", "enriched_accounts"],
                "outputs": ["report"],
                "next_nodes": ["notify_manager"]
            },
            {
                "id": "notify_manager",
                "type": "notify",
                "inputs": ["report"]
            }
        ]
    }
    
    # Create and test planner
    planner = await create_workflow_planner()
    
    # Test different optimization strategies
    strategies = [
        OptimizationStrategy.PERFORMANCE,
        OptimizationStrategy.RESOURCE,
        OptimizationStrategy.COST,
        OptimizationStrategy.RELIABILITY,
        OptimizationStrategy.COMPLIANCE
    ]
    
    for strategy in strategies:
        plan = await planner.create_execution_plan(
            workflow_dsl=sample_workflow,
            optimization_strategy=strategy,
            constraints={"max_parallel_steps": 3},
            tenant_id="1300"
        )
        
        print(f"\nüéØ {strategy.value.upper()} Optimization:")
        print(f"  Plan Hash: {plan.plan_hash[:16]}...")
        print(f"  Steps: {len(plan.execution_steps)}")
        print(f"  Estimated Time: {plan.estimated_total_time_ms}ms")
        print(f"  Parallel Groups: {len(plan.parallel_groups)}")
        print(f"  Resource Requirements: {plan.resource_requirements}")

if __name__ == "__main__":
    asyncio.run(example_usage())
