"""
Enhanced RBA Static Analyzer - Tasks 6.2-T03, T04, T18
Enterprise-grade static analysis for DSL workflows with dynamic validation rules.
Enforces governance, schema compliance, and policy requirements at compile time.

Enhanced Features:
- Multi-severity issue classification (CRITICAL, WARNING, INFO)
- Industry-specific validation rules (SaaS, Banking, Insurance)
- Policy-as-code enforcement with fail-closed design
- Dependency graph validation and cycle detection
- Governance field mandatory validation
- Compliance framework integration
- Performance optimization suggestions
"""

import json
import yaml
import jsonschema
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timezone
import logging
import asyncio
import asyncpg
from pathlib import Path

logger = logging.getLogger(__name__)

class ValidationSeverity(Enum):
    """Validation issue severity levels"""
    ERROR = "error"           # Blocks compilation
    WARNING = "warning"       # Allows compilation with warnings
    INFO = "info"            # Informational only
    CRITICAL = "critical"     # Security/compliance violation

class ValidationCategory(Enum):
    """Categories of validation issues"""
    SCHEMA = "schema"                    # JSON schema violations
    GOVERNANCE = "governance"            # Missing governance fields
    POLICY = "policy"                   # Policy compliance violations
    SECURITY = "security"               # Security concerns
    PERFORMANCE = "performance"         # Performance issues
    BEST_PRACTICES = "best_practices"   # Code quality issues
    TENANT_ISOLATION = "tenant_isolation" # Multi-tenancy violations

@dataclass
class ValidationIssue:
    """Individual validation issue"""
    severity: ValidationSeverity
    category: ValidationCategory
    message: str
    node_id: Optional[str] = None
    field_path: Optional[str] = None
    rule_id: str = ""
    suggestion: Optional[str] = None
    compliance_impact: List[str] = field(default_factory=list)
    
@dataclass
class ValidationResult:
    """Complete validation result"""
    is_valid: bool
    issues: List[ValidationIssue] = field(default_factory=list)
    errors: List[ValidationIssue] = field(default_factory=list)
    warnings: List[ValidationIssue] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def add_issue(self, issue: ValidationIssue):
        """Add validation issue and categorize"""
        self.issues.append(issue)
        if issue.severity in [ValidationSeverity.ERROR, ValidationSeverity.CRITICAL]:
            self.errors.append(issue)
            self.is_valid = False
        elif issue.severity == ValidationSeverity.WARNING:
            self.warnings.append(issue)

class DynamicValidationRuleEngine:
    """
    Dynamic validation rule engine that loads rules from database/config
    Supports tenant-specific and industry-specific validation rules
    """
    
    def __init__(self, db_pool: Optional[asyncpg.Pool] = None):
        self.db_pool = db_pool
        self.validation_rules: Dict[str, Dict] = {}
        self.schema_cache: Dict[str, Dict] = {}
        self.policy_cache: Dict[str, Dict] = {}
        
    async def initialize(self):
        """Initialize validation rules from database and config files"""
        try:
            # Load base validation rules
            await self._load_base_validation_rules()
            
            # Load tenant-specific rules from database
            if self.db_pool:
                await self._load_tenant_validation_rules()
            
            # Load industry overlay rules
            await self._load_industry_validation_rules()
            
            logger.info("‚úÖ Dynamic validation rule engine initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize validation rule engine: {e}")
            # Load minimal fallback rules
            await self._load_fallback_rules()
    
    async def _load_base_validation_rules(self):
        """Load base validation rules from configuration"""
        base_rules = {
            "governance_required_fields": {
                "rule_id": "GOV-001",
                "severity": "error",
                "category": "governance",
                "required_fields": [
                    "policy_id", "tenant_id", "evidence_capture"
                ],
                "message": "Missing required governance field: {field}",
                "compliance_frameworks": ["SOX", "GDPR", "HIPAA"]
            },
            "tenant_isolation_check": {
                "rule_id": "SEC-001", 
                "severity": "critical",
                "category": "tenant_isolation",
                "message": "Workflow must include tenant_id in all data operations",
                "validation_logic": "check_tenant_isolation"
            },
            "policy_compliance_check": {
                "rule_id": "POL-001",
                "severity": "error", 
                "category": "policy",
                "message": "Workflow violates active policy: {policy_name}",
                "validation_logic": "check_policy_compliance"
            },
            "schema_validation": {
                "rule_id": "SCH-001",
                "severity": "error",
                "category": "schema", 
                "message": "Schema validation failed: {details}",
                "validation_logic": "validate_json_schema"
            },
            "performance_optimization": {
                "rule_id": "PERF-001",
                "severity": "warning",
                "category": "performance",
                "message": "Performance concern: {issue}",
                "validation_logic": "check_performance_patterns"
            }
        }
        
        self.validation_rules.update(base_rules)
        logger.info(f"üìã Loaded {len(base_rules)} base validation rules")
    
    async def _load_tenant_validation_rules(self):
        """Load tenant-specific validation rules from database"""
        try:
            async with self.db_pool.acquire() as conn:
                # Load tenant-specific validation rules
                tenant_rules = await conn.fetch("""
                    SELECT tenant_id, validation_rules, compliance_requirements
                    FROM tenant_metadata 
                    WHERE status = 'active'
                """)
                
                for rule in tenant_rules:
                    tenant_id = rule['tenant_id']
                    rules_data = rule['validation_rules'] or {}
                    compliance_reqs = rule['compliance_requirements'] or []
                    
                    # Store tenant-specific rules
                    self.validation_rules[f"tenant_{tenant_id}"] = {
                        "tenant_id": tenant_id,
                        "rules": rules_data,
                        "compliance_requirements": compliance_reqs
                    }
                
                logger.info(f"üìã Loaded validation rules for {len(tenant_rules)} tenants")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not load tenant validation rules: {e}")
    
    async def _load_industry_validation_rules(self):
        """Load industry-specific validation rules"""
        industry_rules = {
            "SAAS": {
                "required_metrics": ["ARR", "MRR", "churn_rate"],
                "compliance_frameworks": ["SOX", "GDPR"],
                "data_retention_days": 2555  # 7 years
            },
            "BANKING": {
                "required_metrics": ["loan_amount", "credit_score", "risk_rating"],
                "compliance_frameworks": ["SOX", "RBI", "BASEL_III"],
                "data_retention_days": 3650  # 10 years
            },
            "INSURANCE": {
                "required_metrics": ["premium", "claim_amount", "solvency_ratio"],
                "compliance_frameworks": ["IRDAI", "GDPR", "HIPAA"],
                "data_retention_days": 2920  # 8 years
            }
        }
        
        self.validation_rules["industry_overlays"] = industry_rules
        logger.info(f"üìã Loaded validation rules for {len(industry_rules)} industries")
    
    async def _load_fallback_rules(self):
        """Load minimal fallback rules if database is unavailable"""
        fallback_rules = {
            "basic_governance": {
                "rule_id": "FALLBACK-001",
                "severity": "error",
                "message": "Basic governance validation failed",
                "required_fields": ["policy_id", "tenant_id"]
            }
        }
        
        self.validation_rules = fallback_rules
        logger.warning("‚ö†Ô∏è Using fallback validation rules")

class RBAStaticAnalyzer:
    """
    Enterprise RBA Static Analyzer - Task 6.2-T03
    
    Features:
    - Dynamic validation rules loaded from database
    - Tenant-specific and industry-specific validation
    - Comprehensive schema validation
    - Governance compliance checking
    - Security and performance analysis
    - Modular and extensible rule system
    """
    
    def __init__(self, db_pool: Optional[asyncpg.Pool] = None):
        self.db_pool = db_pool
        self.rule_engine = DynamicValidationRuleEngine(db_pool)
        self.schema_validator = jsonschema.Draft7Validator({})
        
        # Dynamic validation methods registry
        self.validation_methods = {
            "check_tenant_isolation": self._check_tenant_isolation,
            "check_policy_compliance": self._check_policy_compliance,
            "validate_json_schema": self._validate_json_schema,
            "check_performance_patterns": self._check_performance_patterns,
            "check_security_patterns": self._check_security_patterns,
            "check_governance_completeness": self._check_governance_completeness
        }
        
    async def initialize(self):
        """Initialize the static analyzer"""
        await self.rule_engine.initialize()
        await self._load_dsl_schemas()
        logger.info("‚úÖ RBA Static Analyzer initialized")
    
    async def analyze_workflow(self, workflow_dsl: Dict[str, Any], 
                             tenant_id: str = None, 
                             industry_code: str = None) -> ValidationResult:
        """
        Perform comprehensive static analysis of DSL workflow
        
        Args:
            workflow_dsl: DSL workflow definition
            tenant_id: Tenant ID for tenant-specific validation
            industry_code: Industry code for industry-specific validation
            
        Returns:
            ValidationResult: Complete validation results
        """
        result = ValidationResult(is_valid=True)
        
        try:
            logger.info(f"üîç Starting static analysis for workflow: {workflow_dsl.get('id', 'unknown')}")
            
            # 1. Schema Validation
            await self._validate_schema(workflow_dsl, result)
            
            # 2. Governance Validation
            await self._validate_governance(workflow_dsl, result, tenant_id)
            
            # 3. Policy Compliance Validation
            if tenant_id:
                await self._validate_policy_compliance(workflow_dsl, result, tenant_id)
            
            # 4. Industry-Specific Validation
            if industry_code:
                await self._validate_industry_compliance(workflow_dsl, result, industry_code)
            
            # 5. Security Analysis
            await self._validate_security(workflow_dsl, result)
            
            # 6. Performance Analysis
            await self._validate_performance(workflow_dsl, result)
            
            # 7. Best Practices Check
            await self._validate_best_practices(workflow_dsl, result)
            
            # 8. Tenant Isolation Check
            if tenant_id:
                await self._validate_tenant_isolation(workflow_dsl, result, tenant_id)
            
            # Generate analysis metadata
            result.metadata = {
                "analyzed_at": datetime.now(timezone.utc).isoformat(),
                "workflow_id": workflow_dsl.get("id"),
                "tenant_id": tenant_id,
                "industry_code": industry_code,
                "total_issues": len(result.issues),
                "error_count": len(result.errors),
                "warning_count": len(result.warnings),
                "analyzer_version": "1.0.0"
            }
            
            logger.info(f"‚úÖ Static analysis completed: {len(result.issues)} issues found")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Static analysis failed: {e}")
            result.add_issue(ValidationIssue(
                severity=ValidationSeverity.CRITICAL,
                category=ValidationCategory.SCHEMA,
                message=f"Static analysis failed: {str(e)}",
                rule_id="ANALYZER-ERROR"
            ))
            return result
    
    async def _validate_schema(self, workflow_dsl: Dict[str, Any], result: ValidationResult):
        """Validate workflow against JSON schema"""
        try:
            # Load appropriate schema based on workflow type
            workflow_type = workflow_dsl.get("automation_type", "RBA")
            schema = await self._get_workflow_schema(workflow_type)
            
            # Validate against schema
            validator = jsonschema.Draft7Validator(schema)
            errors = list(validator.iter_errors(workflow_dsl))
            
            for error in errors:
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.ERROR,
                    category=ValidationCategory.SCHEMA,
                    message=f"Schema validation error: {error.message}",
                    field_path=".".join(str(p) for p in error.absolute_path),
                    rule_id="SCH-001"
                ))
                
        except Exception as e:
            result.add_issue(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                category=ValidationCategory.SCHEMA,
                message=f"Schema validation failed: {str(e)}",
                rule_id="SCH-ERROR"
            ))
    
    async def _validate_governance(self, workflow_dsl: Dict[str, Any], result: ValidationResult, tenant_id: str = None):
        """Validate governance requirements"""
        governance_rules = self.rule_engine.validation_rules.get("governance_required_fields", {})
        required_fields = governance_rules.get("required_fields", [])
        
        # Check workflow-level governance
        workflow_governance = workflow_dsl.get("governance", {})
        
        for field in required_fields:
            if field not in workflow_governance:
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.ERROR,
                    category=ValidationCategory.GOVERNANCE,
                    message=f"Missing required governance field: {field}",
                    field_path=f"governance.{field}",
                    rule_id="GOV-001",
                    suggestion=f"Add '{field}' to workflow governance section",
                    compliance_impact=governance_rules.get("compliance_frameworks", [])
                ))
        
        # Check node-level governance
        nodes = workflow_dsl.get("nodes", [])
        for i, node in enumerate(nodes):
            node_governance = node.get("governance", {})
            node_id = node.get("id", f"node_{i}")
            
            for field in required_fields:
                if field not in node_governance and field not in workflow_governance:
                    result.add_issue(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        category=ValidationCategory.GOVERNANCE,
                        message=f"Node missing governance field: {field}",
                        node_id=node_id,
                        field_path=f"nodes[{i}].governance.{field}",
                        rule_id="GOV-002",
                        suggestion=f"Add '{field}' to node governance or inherit from workflow"
                    ))
    
    async def _validate_policy_compliance(self, workflow_dsl: Dict[str, Any], result: ValidationResult, tenant_id: str):
        """Validate against tenant-specific policies"""
        try:
            if not self.db_pool:
                return
                
            async with self.db_pool.acquire() as conn:
                # Get active policies for tenant
                policies = await conn.fetch("""
                    SELECT policy_pack_id, policy_rules, compliance_framework
                    FROM dsl_policy_packs 
                    WHERE tenant_id = $1 AND status = 'active'
                    AND (expiration_date IS NULL OR expiration_date > NOW())
                """, int(tenant_id))
                
                for policy in policies:
                    policy_rules = policy['policy_rules'] or {}
                    
                    # Check execution time limits
                    if 'max_execution_time' in policy_rules:
                        max_time = policy_rules['max_execution_time']
                        estimated_time = self._estimate_execution_time(workflow_dsl)
                        
                        if estimated_time > max_time:
                            result.add_issue(ValidationIssue(
                                severity=ValidationSeverity.ERROR,
                                category=ValidationCategory.POLICY,
                                message=f"Workflow exceeds max execution time: {estimated_time}s > {max_time}s",
                                rule_id="POL-001",
                                compliance_impact=[policy['compliance_framework']]
                            ))
                    
                    # Check required approvals
                    if 'required_approvals' in policy_rules:
                        required_approvals = policy_rules['required_approvals']
                        workflow_approvals = workflow_dsl.get("approvals", [])
                        
                        if len(workflow_approvals) < required_approvals:
                            result.add_issue(ValidationIssue(
                                severity=ValidationSeverity.ERROR,
                                category=ValidationCategory.POLICY,
                                message=f"Insufficient approvals: {len(workflow_approvals)} < {required_approvals}",
                                rule_id="POL-002",
                                suggestion="Add required approval steps to workflow"
                            ))
                            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Policy compliance validation failed: {e}")
    
    async def _validate_industry_compliance(self, workflow_dsl: Dict[str, Any], result: ValidationResult, industry_code: str):
        """Validate industry-specific requirements"""
        industry_rules = self.rule_engine.validation_rules.get("industry_overlays", {}).get(industry_code, {})
        
        if not industry_rules:
            return
        
        # Check required metrics
        required_metrics = industry_rules.get("required_metrics", [])
        workflow_metrics = self._extract_workflow_metrics(workflow_dsl)
        
        for metric in required_metrics:
            if metric not in workflow_metrics:
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    category=ValidationCategory.POLICY,
                    message=f"Missing industry-required metric: {metric}",
                    rule_id=f"IND-{industry_code}-001",
                    suggestion=f"Add {metric} metric tracking for {industry_code} compliance"
                ))
        
        # Check data retention requirements
        retention_days = industry_rules.get("data_retention_days")
        workflow_retention = workflow_dsl.get("data_retention_days")
        
        if workflow_retention and workflow_retention < retention_days:
            result.add_issue(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                category=ValidationCategory.POLICY,
                message=f"Data retention period too short: {workflow_retention} < {retention_days} days",
                rule_id=f"IND-{industry_code}-002",
                compliance_impact=industry_rules.get("compliance_frameworks", [])
            ))
    
    async def _validate_security(self, workflow_dsl: Dict[str, Any], result: ValidationResult):
        """Validate security patterns and requirements"""
        # Check for sensitive data handling
        nodes = workflow_dsl.get("nodes", [])
        
        for i, node in enumerate(nodes):
            node_id = node.get("id", f"node_{i}")
            node_type = node.get("type", "")
            params = node.get("params", {})
            
            # Check for hardcoded credentials
            if self._contains_hardcoded_credentials(params):
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.CRITICAL,
                    category=ValidationCategory.SECURITY,
                    message="Hardcoded credentials detected",
                    node_id=node_id,
                    rule_id="SEC-001",
                    suggestion="Use secure credential management (environment variables, vault)"
                ))
            
            # Check for PII handling without encryption
            if self._handles_pii_without_encryption(node):
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.ERROR,
                    category=ValidationCategory.SECURITY,
                    message="PII handling without encryption",
                    node_id=node_id,
                    rule_id="SEC-002",
                    compliance_impact=["GDPR", "HIPAA"]
                ))
    
    async def _validate_performance(self, workflow_dsl: Dict[str, Any], result: ValidationResult):
        """Validate performance patterns"""
        nodes = workflow_dsl.get("nodes", [])
        
        # Check for potential performance issues
        for i, node in enumerate(nodes):
            node_id = node.get("id", f"node_{i}")
            node_type = node.get("type", "")
            
            # Check for inefficient query patterns
            if node_type == "query" and self._has_inefficient_query_pattern(node):
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    category=ValidationCategory.PERFORMANCE,
                    message="Potentially inefficient query pattern detected",
                    node_id=node_id,
                    rule_id="PERF-001",
                    suggestion="Consider adding indexes or optimizing query structure"
                ))
            
            # Check for excessive loops
            if self._has_excessive_loops(node):
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    category=ValidationCategory.PERFORMANCE,
                    message="Excessive loop iterations detected",
                    node_id=node_id,
                    rule_id="PERF-002",
                    suggestion="Consider batch processing or pagination"
                ))
    
    async def _validate_best_practices(self, workflow_dsl: Dict[str, Any], result: ValidationResult):
        """Validate coding best practices"""
        # Check naming conventions
        workflow_name = workflow_dsl.get("name", "")
        if not self._follows_naming_convention(workflow_name):
            result.add_issue(ValidationIssue(
                severity=ValidationSeverity.INFO,
                category=ValidationCategory.BEST_PRACTICES,
                message="Workflow name doesn't follow naming convention",
                rule_id="BP-001",
                suggestion="Use descriptive, snake_case naming"
            ))
        
        # Check for proper error handling
        nodes = workflow_dsl.get("nodes", [])
        nodes_without_error_handling = []
        
        for node in nodes:
            if not node.get("error_handlers") and node.get("type") in ["query", "agent_call"]:
                nodes_without_error_handling.append(node.get("id", "unknown"))
        
        if nodes_without_error_handling:
            result.add_issue(ValidationIssue(
                severity=ValidationSeverity.WARNING,
                category=ValidationCategory.BEST_PRACTICES,
                message=f"Nodes without error handling: {', '.join(nodes_without_error_handling)}",
                rule_id="BP-002",
                suggestion="Add error handlers for external operations"
            ))
    
    async def _validate_tenant_isolation(self, workflow_dsl: Dict[str, Any], result: ValidationResult, tenant_id: str):
        """Validate tenant isolation requirements"""
        nodes = workflow_dsl.get("nodes", [])
        
        for i, node in enumerate(nodes):
            node_id = node.get("id", f"node_{i}")
            node_type = node.get("type", "")
            
            # Check data operations include tenant_id
            if node_type == "query" and not self._includes_tenant_filter(node, tenant_id):
                result.add_issue(ValidationIssue(
                    severity=ValidationSeverity.CRITICAL,
                    category=ValidationCategory.TENANT_ISOLATION,
                    message="Query operation missing tenant isolation",
                    node_id=node_id,
                    rule_id="TEN-001",
                    suggestion="Add tenant_id filter to all data operations"
                ))
    
    # Helper methods for validation logic
    def _estimate_execution_time(self, workflow_dsl: Dict[str, Any]) -> int:
        """Estimate workflow execution time in seconds"""
        nodes = workflow_dsl.get("nodes", [])
        estimated_time = 0
        
        for node in nodes:
            node_type = node.get("type", "")
            # Simple estimation based on node type
            if node_type == "query":
                estimated_time += 2
            elif node_type == "agent_call":
                estimated_time += 10
            elif node_type == "ml_decision":
                estimated_time += 5
            else:
                estimated_time += 1
        
        return estimated_time
    
    def _extract_workflow_metrics(self, workflow_dsl: Dict[str, Any]) -> Set[str]:
        """Extract metrics tracked by workflow"""
        metrics = set()
        nodes = workflow_dsl.get("nodes", [])
        
        for node in nodes:
            params = node.get("params", {})
            # Look for metric references in queries and outputs
            if "metrics" in params:
                metrics.update(params["metrics"])
        
        return metrics
    
    def _contains_hardcoded_credentials(self, params: Dict[str, Any]) -> bool:
        """Check for hardcoded credentials in parameters"""
        sensitive_keys = ["password", "api_key", "secret", "token", "credential"]
        
        def check_dict(d):
            if isinstance(d, dict):
                for key, value in d.items():
                    if any(sensitive in key.lower() for sensitive in sensitive_keys):
                        if isinstance(value, str) and not value.startswith("${"):
                            return True
                    if isinstance(value, (dict, list)):
                        if check_dict(value):
                            return True
            elif isinstance(d, list):
                for item in d:
                    if check_dict(item):
                        return True
            return False
        
        return check_dict(params)
    
    def _handles_pii_without_encryption(self, node: Dict[str, Any]) -> bool:
        """Check if node handles PII without encryption"""
        pii_indicators = ["email", "phone", "ssn", "credit_card", "personal"]
        params = node.get("params", {})
        
        # Simple check for PII field references
        params_str = json.dumps(params).lower()
        has_pii = any(indicator in params_str for indicator in pii_indicators)
        has_encryption = "encrypt" in params_str or "hash" in params_str
        
        return has_pii and not has_encryption
    
    def _has_inefficient_query_pattern(self, node: Dict[str, Any]) -> bool:
        """Check for inefficient query patterns"""
        params = node.get("params", {})
        query = params.get("query", "").upper()
        
        # Check for common inefficient patterns
        inefficient_patterns = [
            "SELECT *",           # Select all columns
            "WHERE LIKE '%",      # Leading wildcard
            "ORDER BY" not in query and "LIMIT" in query,  # Limit without order
        ]
        
        return any(pattern in query if isinstance(pattern, str) else pattern for pattern in inefficient_patterns)
    
    def _has_excessive_loops(self, node: Dict[str, Any]) -> bool:
        """Check for excessive loop iterations"""
        params = node.get("params", {})
        
        # Check for loop configurations
        if "loop" in params:
            max_iterations = params["loop"].get("max_iterations", 0)
            return max_iterations > 1000
        
        return False
    
    def _follows_naming_convention(self, name: str) -> bool:
        """Check if name follows naming convention"""
        # Simple check for snake_case and descriptive names
        import re
        return bool(re.match(r'^[a-z][a-z0-9_]*[a-z0-9]$', name)) and len(name) > 3
    
    def _includes_tenant_filter(self, node: Dict[str, Any], tenant_id: str) -> bool:
        """Check if query includes tenant isolation"""
        params = node.get("params", {})
        query = params.get("query", "")
        filters = params.get("filters", {})
        
        # Check for tenant_id in query or filters
        return "tenant_id" in query.lower() or "tenant_id" in filters
    
    async def _load_dsl_schemas(self):
        """Load DSL schemas for validation"""
        # This would load schemas from files or database
        # For now, using a basic schema
        self.dsl_schema = {
            "type": "object",
            "required": ["id", "name", "nodes"],
            "properties": {
                "id": {"type": "string"},
                "name": {"type": "string"},
                "automation_type": {"enum": ["RBA", "RBIA", "AALA"]},
                "nodes": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["id", "type"],
                        "properties": {
                            "id": {"type": "string"},
                            "type": {"enum": ["query", "decision", "ml_decision", "agent_call", "notify", "governance"]},
                            "governance": {
                                "type": "object",
                                "required": ["policy_id", "tenant_id"]
                            }
                        }
                    }
                }
            }
        }
    
    async def _get_workflow_schema(self, workflow_type: str) -> Dict[str, Any]:
        """Get schema for specific workflow type"""
        return self.dsl_schema  # For now, return base schema

# Factory function for creating analyzer instances
async def create_static_analyzer(db_pool: Optional[asyncpg.Pool] = None) -> RBAStaticAnalyzer:
    """
    Factory function to create and initialize static analyzer
    
    Args:
        db_pool: Database connection pool for dynamic rule loading
        
    Returns:
        Initialized RBAStaticAnalyzer instance
    """
    analyzer = RBAStaticAnalyzer(db_pool)
    await analyzer.initialize()
    return analyzer

# Example usage and testing
async def example_usage():
    """Example usage of the static analyzer"""
    
    # Sample workflow for testing
    sample_workflow = {
        "id": "sample_pipeline_hygiene",
        "name": "pipeline_hygiene_workflow",
        "automation_type": "RBA",
        "governance": {
            "policy_id": "saas_pipeline_policy_v1",
            "tenant_id": "1300",
            "evidence_capture": True,
            "compliance_tags": ["SOX", "GDPR"]
        },
        "nodes": [
            {
                "id": "fetch_opportunities",
                "type": "query",
                "governance": {
                    "policy_id": "data_access_policy",
                    "tenant_id": "1300"
                },
                "params": {
                    "source": "salesforce",
                    "query": "SELECT * FROM Opportunity WHERE tenant_id = '1300'",
                    "filters": {"tenant_id": "1300"}
                }
            },
            {
                "id": "notify_manager",
                "type": "notify",
                "governance": {
                    "policy_id": "notification_policy",
                    "tenant_id": "1300"
                },
                "params": {
                    "channel": "slack",
                    "message": "Pipeline hygiene check completed"
                }
            }
        ]
    }
    
    # Create and test analyzer
    analyzer = await create_static_analyzer()
    
    # Analyze workflow
    result = await analyzer.analyze_workflow(
        workflow_dsl=sample_workflow,
        tenant_id="1300",
        industry_code="SAAS"
    )
    
    print(f"‚úÖ Analysis completed: Valid={result.is_valid}")
    print(f"üìä Issues found: {len(result.issues)}")
    print(f"‚ùå Errors: {len(result.errors)}")
    print(f"‚ö†Ô∏è Warnings: {len(result.warnings)}")
    
    for issue in result.issues:
        print(f"  {issue.severity.value.upper()}: {issue.message}")

if __name__ == "__main__":
    asyncio.run(example_usage())
