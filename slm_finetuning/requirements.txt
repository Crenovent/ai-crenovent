# SLM Fine-tuning Requirements
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
peft>=0.6.0
bitsandbytes>=0.41.0
accelerate>=0.24.0
sentencepiece>=0.1.99
protobuf>=3.20.0
numpy>=1.24.0
tqdm>=4.64.0

# Optional: for better performance
flash-attn>=2.3.0  # For faster training (if compatible GPU)
deepspeed>=0.10.0  # For distributed training

